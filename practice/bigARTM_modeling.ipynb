{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематическое моделирование. BigARTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Необходимо установить библиотеку BigARTM: http://docs.bigartm.org/en/latest/installation/linux.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "# Prepare data\n",
    "# Case 1: data in CountVectorizer format\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['talk.politics.misc', 'talk.religion.misc', 'comp.graphics', 'sci.space', 'rec.autos']\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "        'talk.politics.misc',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "        'rec.autos',\n",
    "    ]\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                                shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # stem the words\n",
    "    # porter = PorterStemmer()\n",
    "    # words = [porter.stem(word) for word in words]\n",
    "    # lemmatize\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing...\n",
      "4353 texts were preprocessed\n"
     ]
    }
   ],
   "source": [
    "print(\"Text preprocessing...\")\n",
    "for i in range(len(dataset.data)):\n",
    "    dataset.data[i] = preprocess_text(dataset.data[i])\n",
    "print(f'{i+1} texts were preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=1000, stop_words='english')\n",
    "n_wd = array(cv.fit_transform(dataset.data).todense()).T\n",
    "vocabulary = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
    "                          n_wd=n_wd,\n",
    "                          vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['image',\n",
       "  'jpeg',\n",
       "  'file',\n",
       "  'images',\n",
       "  'available',\n",
       "  'graphics',\n",
       "  'software',\n",
       "  'data',\n",
       "  'files',\n",
       "  'format'],\n",
       " ['nt',\n",
       "  'like',\n",
       "  'think',\n",
       "  'good',\n",
       "  'problem',\n",
       "  'time',\n",
       "  'people',\n",
       "  'make',\n",
       "  'way',\n",
       "  'really'],\n",
       " ['nt',\n",
       "  'president',\n",
       "  'mr',\n",
       "  'think',\n",
       "  'going',\n",
       "  'know',\n",
       "  'stephanopoulos',\n",
       "  'people',\n",
       "  'said',\n",
       "  'ms'],\n",
       " ['space',\n",
       "  'car',\n",
       "  'new',\n",
       "  'earth',\n",
       "  'launch',\n",
       "  'nasa',\n",
       "  'cars',\n",
       "  'shuttle',\n",
       "  'orbit',\n",
       "  'mission'],\n",
       " ['people',\n",
       "  'nt',\n",
       "  'god',\n",
       "  'jesus',\n",
       "  'say',\n",
       "  'know',\n",
       "  'said',\n",
       "  'life',\n",
       "  'believe',\n",
       "  'children']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn simple LDA model (or you can use advanced artm.ARTM)\n",
    "model = artm.LDA(num_topics=5, dictionary=bv.dictionary)\n",
    "model.fit_offline(bv, num_collection_passes=20)\n",
    "\n",
    "# Print results\n",
    "model.get_top_tokens()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
